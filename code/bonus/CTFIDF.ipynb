{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CTFIDF.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDuI8EwJq_66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1fbbd21-2e29-42c8-c99a-f24596035724"
      },
      "source": [
        "import sys\n",
        "import shutil\n",
        "import re\n",
        "import math\n",
        "from collections import Counter\n",
        "# shutil.rmtree('TF_index')\n",
        "# !zip -r TF_index.zip CTF_index\n",
        "# !unzip TF_index.zip\n",
        "!pip install pyspark\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# create Spark context with necessary configuration\n",
        "conf = SparkConf()\n",
        "conf.setMaster('local')\n",
        "conf.setAppName('TF/IDF')\n",
        "sc = SparkContext.getOrCreate(conf=conf)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.1.1)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3wRJSBNxedB"
      },
      "source": [
        "# read data\n",
        "# Edit file name to give your input data\n",
        "dataFile = '2.txt'\n",
        "stopWordFile = 'stopwords-en.txt'\n",
        "# create RDDS, read stopwords file\n",
        "fileRDD = sc.textFile(dataFile)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk-0ytxepXL-"
      },
      "source": [
        "# Remove punctutaion function\n",
        "\n",
        "def lower_clean_str(x):\n",
        "  punc='!\"#$%&\\'()*+,;=?@[]^_`{|}~-'\n",
        "  # punc='!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~-'\n",
        "  lowercased_str = x.lower()\n",
        "  for ch in punc:\n",
        "    lowercased_str = lowercased_str.replace(ch, ' ')\n",
        "    lowercased_str = ' '.join(lowercased_str.split())\n",
        "  return lowercased_str\n",
        "def lower_clean_str1(x):\n",
        "  # punc='!\"#$%&\\'()*+,;=?@[]^_`{|}~-'\n",
        "  punc='!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~-'\n",
        "  lowercased_str = x.lower()\n",
        "  for ch in punc:\n",
        "    lowercased_str = lowercased_str.replace(ch, ' ')\n",
        "    lowercased_str = ' '.join(lowercased_str.split())\n",
        "  return lowercased_str\n",
        "\n",
        "#Run remove punctuation function on data\n",
        "fileRDD = sc.textFile(dataFile)\n",
        "fileRDD = fileRDD.map(lower_clean_str)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl4JTP3DqLTe"
      },
      "source": [
        "# Remove urls\n",
        "def remove_urls_tags(x):\n",
        "  regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
        "  # regex = 'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
        "  url_str = re.sub(regex, \" \", x)\n",
        "  html_regex = r\"<.*?>\"\n",
        "  finalStr = re.sub(html_regex, \" \", url_str)\n",
        "  return finalStr\n",
        "\n",
        "fileRDD = fileRDD.map(remove_urls_tags)\n",
        "fileRDD = fileRDD.map(lower_clean_str1)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhOCrxhW_gWU"
      },
      "source": [
        "# Remove stop words function\n",
        "def remove_stop_words(stri):\n",
        "  with open(stopWordFile,'r') as stop:\n",
        "    stop_w = stop.read()\n",
        "    #splitting stopwords.txt by new line\n",
        "    stop_list = re.split('\\n', stop_w)\n",
        "    for word in stop_list:\n",
        "        if word in stri:\n",
        "            stri = stri.replace(\" \"+ word + \" \", \" \")\n",
        "    return stri\n",
        "    \n",
        "fileRDD1 = fileRDD.map(remove_stop_words)\n",
        "fileRDD1 = fileRDD1.map(lambda line : line.strip())\n",
        "# fileRDD1.take(10)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oze-gVgYsKdY"
      },
      "source": [
        "def split_file(line):\n",
        "    spl = line.split(\" \", 1)\n",
        "    return (spl[0], spl[1])\n",
        "dfRDD = fileRDD1.map(split_file)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfkxFdPcDjSL"
      },
      "source": [
        "# 1. construct the above RDD into: Construct TF\n",
        "# calculating log-weighted term frequencies of each word (w)\n",
        "def tf_construct(line):\n",
        "    res = []\n",
        "    skip = ['']\n",
        "    words = [word for word in line[1].split(\" \") if word not in ['']]\n",
        "    counter  = Counter(words)\n",
        "    result = [(k, (line[0], 1+math.log10(v))) for k, v in counter.items()] \n",
        "    return result\n",
        "\n",
        "tf = dfRDD.flatMap(tf_construct)\n",
        "tfReduce = tf.groupByKey().map(lambda x : (x[0],list(x[1])))\n",
        "# tfReduce.take(2)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDJZFuoiFNRB"
      },
      "source": [
        "# calculate IDF\n",
        "N = fileRDD1.count()\n",
        "prd3 = tfReduce.map(lambda x : (x[0], len(x[1])))\n",
        "IDF=prd3.map(lambda x: (x[0],math.log10(N/(1+x[1]))))\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coKmJFZxG8i6"
      },
      "source": [
        "# Join tf and IDF together\n",
        "joinTFIDF = tfReduce.join(IDF)\n",
        "# joinTFIDF.take(5)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQTf1c_PHYWN"
      },
      "source": [
        "# 2. Now multiply each freq value with idf value to get TF-IDF score of each word\n",
        "# which transform the RDD into:\n",
        "# (word, ([(doc-1, tf-idf), (doc-2, tf-idf), (doc-3, tf-idf), ...]))\n",
        "def extract_join(jd):\n",
        "    wrd = jd[0]\n",
        "    lis = jd[1][0]\n",
        "    idf = jd[1][1]\n",
        "    li =[]\n",
        "    for x in lis:\n",
        "      li.append((x[0], ((float)(x[1]) *(float)(idf))))\n",
        "    return (wrd,li)\n",
        "\n",
        "tfIDFFinal = joinTFIDF.map(extract_join)\n",
        "# tfIDFFinal.take(2)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymq0AJX_I6fn"
      },
      "source": [
        "# 3.  Follow the same procedure given in Task – 2 to cosine normalize the construct the normalized TFIDF index of format:\n",
        "# (word, ([(doc-1, weighted_tf-idf), (doc-2, weighted_tf-idf), ...]))\n",
        "\n",
        "def tf_construct(line):\n",
        "    mul = [(k, v*v) for k,v in line[1]]\n",
        "    li = [v[1] for v in mul]\n",
        "    eucl=math.sqrt(sum(li))\n",
        "    result = [(line[0], (str(k) + \"#\"+ str(v/eucl))) for k, v in mul]\n",
        "    return result\n",
        "\n",
        "tf = tfIDFFinal.flatMap(tf_construct)\n",
        "CTFIDF_indexRDD = tf.groupByKey().map(lambda x : (x[0]+\"@\"+\"+\".join(list(x[1]))))\n",
        "# CTFIDF_indexRDD.take(5)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-WH8GrqMzwT"
      },
      "source": [
        "####################### 4.  Write this RDD to CTFIDF_index directory.\n",
        "CTFIDF_indexRDD.saveAsTextFile('CTFIDF_index')"
      ],
      "execution_count": 18,
      "outputs": []
    }
  ]
}